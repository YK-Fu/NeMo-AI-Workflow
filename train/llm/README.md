# NeMo LLM Training Examples

This directory contains examples and resources for training Large Language Models (LLMs) using NVIDIA's NeMo framework.

## Continual Pre-training and Supervised Fine-tuning

For comprehensive examples of continual pre-training and supervised fine-tuning with NeMo, please refer to the [NeMo-Tutorial](https://github.com/wcks13589/NeMo-Tutorial/) repository.

The NeMo-Tutorial repository provides:
- Step-by-step guides for continual pre-training
- Examples of supervised fine-tuning
- Best practices for model training
- Code samples and configurations

## Getting Started

1. Clone the NeMo-Tutorial repository:
```bash
git clone https://github.com/wcks13589/NeMo-Tutorial.git
```

2. Follow the instructions in the repository for:
   - Setting up your environment
   - Preparing your dataset
   - Running training scripts

## Additional Resources

- [NeMo Documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/)
- [NeMo GitHub Repository](https://github.com/NVIDIA/NeMo)
- [NVIDIA NGC Containers](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo) 